
---
layout: post
title: Dia 3 de 100 - Web Scraping com Python - Parte 2 de 3
---

Bom, primeiramente gostaria de lamentar profundamente por meu afastamento peri√≥dico da a atividades do #100daysofcode. Estou em uma situa√ß√£o mista de p√≥s-operat√≥rio e de pico de condi√ß√£o neuropsiqui√°trica transit√≥ria, somado a alguns outros estressores externos e a alguns jobs pesados que surgiram - e que foram conclu√≠dos com sucesso. Mas, como o desafio n√£o pode parar, desde j√° combinamos que, por mais que haja espa√ßamento maior entre postagens, a contagem seguir√° de 1 a 100 normalmente.

Para hoje (dia 3 de 100), seguimos na parte 2 sobre web scraping - agora com uma abordagem mais pr√°tica, por√©m simulada (leia-se, um ambiente prop√≠cio p√£o aprendizado). A parte 3 ser√° uma aplica√ß√£o "real" dos dados aqui obtidos para infer√™ncia estat√≠stica. Havendo possibilidade, posteriormente faremos uma parte b√¥nus com um hands-on "diferente". üòú

### Dados utilizados

Uma boa alternativa para o ambiente "simulado" s√£o as se√ß√µes de datasets do [Kaggle](https://www.kaggle.com/datasets) e do [Sports Reference](https://www.sports-reference.com). Em ambas, est√£o dispon√≠veis boas quantidades de dados em condi√ß√µes para manipula√ß√£o na forma como desejamos. Especialmente no caso do Sports Reference, h√° v√°rios tipos de estat√≠sticas, desde as mais simples at√© as avan√ßadas, sobre Basquete Futebol Americano e outros, do universit√°rio ao profissional das mais diferentes ligas. Esta ser√°, pois, a base utilizada neste experimento "acad√™mico" e, a seguir, h√° uma captura de tela para sua visualiza√ß√£o.

![img1](images/d3of100_img1.jpg)
_Datasets do site Sports Reference_

Como exemplo, usaremos o dataset de Basquete para extrair dados da NBA. Contudo, o conte√∫do aqui transmitido possibilitar√° a voc√™ reproduzir o procedimento com poucas modifica√ß√µes em qualquer outro dataset, uma vez que todos pertencem √† mesma empresa e possuem o mesmo padr√£o.

### Bibliotecas utilizadas

* `requests`: para para execu√ß√£o de requisi√ß√µes HTTP;
* `BeautifulSoup`: para extra√ß√£o de dados em arquivos HTML e XML; e
* `Pandas`: para armazenar, limpar e salvar os dados em forma de tabela.

H√° algumas outras formas distintas de realizar os processos aqui apresentados (por exemplo, via `Selenium` e simulando requisi√ß√µes por uma p√°gina com intera√ß√£o simulada no _Chromium_), mas abordo aqui esta alternativa por seu desempenho otimizado.

Desta forma, usaremos a biblioteca `requests` para executar requisi√ß√µes GET e obter o c√≥digo HTML das p√°ginas que queremos, depois, utilizaremos a `BeautifulSoup` para extrair os dados que queremos destas p√°ginas e, por fim, salvaremos esses dados em um dataframe do `Pandas`.

### Extra√ß√£o de dados

Como primeiro exemplo, vamos utilizar a p√°gina de [Estat√≠sticas Individuais Totais da temporada de 2018/2019 da NBA](https://www.basketball-reference.com/leagues/NBA_2019_totals.html). Esta p√°gina cont√©m uma √∫nica tabela, onde cada linha representa um jogador, seguido de suas informa√ß√µes na temporada da NBA de interesse, como por exemplo a quantidade de minutos jogados e arremessos convertidos.

Utilizando a ferramenta de inspe√ß√£o do navegador na tabela em quest√£o para analisar a estrutura√ß√£o dos dados, verifica-se que eles est√£o armazenados em uma tabela HTML normal, representado pela tag `table`. Sabendo-se qual elemento devemos extrair da p√°gina HTML para conseguir os dados desejados, _it's time for hands on!_

![img2](images/d3of100_img2.jpg)
_Inspecionando o c√≥digo-fonte para identificar a estrutura√ß√£o dos dados_

O primeiro passo √©, claramente, importar a bibliotecas que ser√£o usadas:

```python
import pandas
import requests
from bs4 import BeautifulSoup
```

Logo ap√≥s, faremos uma requisi√ß√£o GET para a p√°gina. Para isso, devemos chamar o m√©todo `requests.get` com a URL da p√°gina como argumento.

```python
req = requests.get('https://www.basketball-reference.com/leagues/NBA_2019_totals.html')

if req.status_code == 200:
    print('Requisi√ß√£o bem sucedida!')
    content = req.content
```

Ap√≥s obter o c√≥digo HTML da p√°gina, podemos utilizar a biblioteca `BeautifulSoup` para extrair a tabela. Primeiro, devemos criar um objeto que onde o documento ser√° armazenado de forma estruturada (de acordo com as _tags_) e depois poderemos acessar o elemento que desejarmos, chamando o m√©todo `find` e passando como argumento o nome da _tag_ desejada - neste caso, `table`. Segue:

```python
soup = BeautifulSoup(content, 'html.parser')
table = soup.find(name='table')
```

Uma vez armazenado o c√≥digo HTML da tabela, podemos utilizar o `Pandas` para carregar os dados em um _dataframe_ utilizando o m√©todo `read_html`. Para tanto, h√° dois pontos que demandam aten√ß√£o em dobro: o primeiro √© que, antes de passar a vari√°vel _table_ na fun√ß√£o, devemos convert√™-la para _string,_ haja vista que, no momento, ela √© um objeto do _BeautifulSoup;_ o segundo ponto √© que o retorno deste m√©todo √© sempre uma lista de _dataframes_ e, portanto, devemos acessar sua _posi√ß√£o 0_ para obter a tabela.

```python
table_str = str(table)
df = pd.read_html(table_str))[0]
```

Faz-se mister frisar, ainda, uma nova particularidade: no caso corrente, a p√°gina trabalhada cont√©m apenas uma tabela, havendo t√£o somente uma tag _table_ para extrair. No entanto, em casos onde existam v√°rias tabelas, para obter dados de uma tabela espec√≠fica existem duas op√ß√µes:

1. Substituir o m√©todo `find` pelo `find_all`, o qual retorna uma lista de todos os elementos encontrados, e acessar a tabela desejada verificando em qual posi√ß√£o do vetor ela se encontra.
2. Utilizar o argumento `attrs` do m√©todo `find`, passando um dicion√°rio que indica quais atributos o elemento deve ter para ser extra√≠do. Por exemplo, considerando a p√°gina de classifica√ß√µes em vez da p√°gina atual, √© imaginando que queremos extrair as coloca√ß√µes dos times na confer√™ncia Oeste _(Western Conference),_ usamos o inspetor para verificar que o `id` dessa tabela √© _‚Äúconfs_standings_W‚Äù._ Portanto, o c√≥digo ficaria da seguinte forma:

```python
table = soup.find(name='table', attrs={'id':'confs_standings_W'})
```

### Extraindo dados de m√∫ltiplas temporadas

Como todas as p√°ginas que exibem os dados de cada temporada s√£o baseadas em um mesmo _template,_ uma solu√ß√£o fact√≠vel e otimizada √© criar um _loop_ que tiete sobre uma lista de anos, repetindo o processo para cada um destes.

Destarte, o c√≥digo a seguir define uma fun√ß√£o que automatiza o processo e extrai as estat√≠sticas totais de 2015 a 2019 (escolha propria). Note que uma coluna _Year_ √© criada em cada extra√ß√£o para que seja poss√≠vel diferenciar de qual ano cada estat√≠stica pertence no _dataframe_ principal.

```python
def scrape_stats(base_url, year_start, year_end):
    years = range(year_start,year_end+1,1)

    final_df = pd.DataFrame()

    for year in years:
        print('Extraindo ano {}'.format(year))
        req_url = base_url.format(year)
        req = requests.get(req_url)
        soup = BeautifulSoup(req.content, 'html.parser')
        table = soup.find('table', {'id':'totals_stats'})
        df = pd.read_html(str(table))[0]
        df['Year'] = year

    return final_df

url = 'https://www.basketball-reference.com/leagues/NBA_{}_totals.html'

df = scrape_stats(url, 2015, 2019)
```

E pronto! Temos armazenado em _df_ um _dataframe_  com 

### Conclus√£o parcial

Bom, neste ponto j√° estamos com nossos dados baixados e armazenados no _dataframe_ `df` e, na pr√≥xima parte desta s√©rie, manipularemos este _dataframe_ para extrair informa√ß√µes de interesse e mostrar exemplos de como usar e analisar estatisticamente seus resultados.

#100daysofcode #day3of100 #programming #coding #code #python #developer #coder #programmer #peoplewhocode #hacking #ethicalhacking #hacktheworld #webscraping
